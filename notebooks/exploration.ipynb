{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxgwIW6br_8c"
   },
   "source": [
    "# MechIR Activation Patching\n",
    "\n",
    "This notebook serves to walk you through a simple example of activation patching in `MechIR`. For more details on the specifics of this process check out our other notebook!.\n",
    "\n",
    "NOTE: Our initial implementation is flexible enough to support loading a wide variety of Transformer-based IR models, but we have mainly tested the following models:\n",
    "- TAS-B (bi-encoder) [[HofstÃ¤tter et al.]](https://arxiv.org/abs/2104.06967) [[HF model card]](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco)\n",
    "- monoELECTRA (cross-encoder) [[Pradeep et al.]](https://link.springer.com/chapter/10.1007/978-3-030-99736-6_44) [[HF model card]](https://huggingface.co/crystina-z/monoELECTRA_LCE_nneg31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_F6EgFvTr_8d"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mLCPXacr_8d",
    "outputId": "83496403-e207-4d18-e378-2ad133bf57dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/cchen207/git/brown/research/MechIR\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mechir\n",
      "  Building wheel for mechir (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mechir: filename=mechir-0.0.1-py3-none-any.whl size=66580 sha256=4241b7eb8845a4555551762d163cda108fb4b63b267c325fdd617e1d76d6602b\n",
      "  Stored in directory: /private/var/folders/w8/j9c1qwbx3cn8hf10x5nz_xtr0000gp/T/pip-ephem-wheel-cache-ugqeu0x0/wheels/39/37/7c/9f04c1e8f880bc1e666f79cde17d9e585bcff18fdf2b5a9b0d\n",
      "Successfully built mechir\n",
      "Installing collected packages: mechir\n",
      "  Attempting uninstall: mechir\n",
      "    Found existing installation: mechir 0.0.1\n",
      "    Uninstalling mechir-0.0.1:\n",
      "      Successfully uninstalled mechir-0.0.1\n",
      "Successfully installed mechir-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformer_lens in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.34.2)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (3.0.1)\n",
      "Requirement already satisfied: einops>=0.6.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.2.34)\n",
      "Requirement already satisfied: numpy>=1.26 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (13.9.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (2.4.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (4.45.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformer_lens) (0.18.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.10.8)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (75.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.20.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.28.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.15.0)\n",
      "Requirement already satisfied: setproctitle in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/mechir/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q git+https://github.com/Parry-Parry/MechIR.git\n",
    "%pip install -q transformer_lens\n",
    "%pip install -q matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1zmJUINr_8e",
    "outputId": "8cb4185e-d3e7-4864-800d-0d59454b22dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mechir/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mechir import Dot, Cat\n",
    "from mechir.data import MechIRDataset, DotDataCollator, CatDataCollator\n",
    "from mechir.perturb import perturbation\n",
    "from mechir.plotting import plot_components\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxPp15vmr_8e"
   },
   "source": [
    "## Load Model\n",
    "\n",
    "* `Dot` : A bi-encoder architecture with flexibility for different BERT architectures and pooling forms.\n",
    "* `Cat` : A cross-encoder architecture with with flexibility for different BERT architectures. Checkout our `monoT5` class if you want to work with sequence-to-sequence models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JNgLgOpr_8e",
    "outputId": "a654dcb5-0f12-4204-c2d6-33d604fa45b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for BERT in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using BERT for interpretability research, keep in mind that BERT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco into HookedEncoder\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "model = Dot(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SvOoLZUr_8f"
   },
   "outputs": [],
   "source": [
    "cat_model_name = \"crystina-z/monoELECTRA_LCE_nneg31\"\n",
    "cat_model = Cat(cat_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gugeJ5hbr_8f"
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "We recommend the use of `ir-datasets` as it is the easiest way to get started with MechIR. By default `MechIR` will load relevance judgements from these datasets however you can change this or even use your own documents and queries using the `MechDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s20bffar_8g",
    "outputId": "2ea72393-f32a-4577-e66a-fa3063801bed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1502</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4462</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4569</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id doc_id  relevance iteration\n",
       "0        1   1239          1         0\n",
       "1        1   1502          1         0\n",
       "2        1   4462          1         0\n",
       "3        1   4569          1         0\n",
       "4        1   5472          1         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load smallest dataset for quick testing\n",
    "dataset = MechIRDataset(\"vaswani\")\n",
    "dataset.pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDhYqYk4r_8g",
    "outputId": "71a93c7d-9de3-462c-f4ba-028b2a0719b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries in dataset: 93\n",
      "\n",
      "----------- Examples of queries: -----------\n",
      "\n",
      "MEASUREMENT OF DIELECTRIC CONSTANT OF LIQUIDS BY THE USE OF MICROWAVE TECHNIQUES\n",
      "\n",
      "MATHEMATICAL ANALYSIS AND DESIGN DETAILS OF WAVEGUIDE FED MICROWAVE RADIATIONS\n",
      "\n",
      "USE OF DIGITAL COMPUTERS IN THE DESIGN OF BAND PASS FILTERS HAVING GIVEN PHASE AND ATTENUATION CHARACTERISTICS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print examples of queries\n",
    "print(\"Total queries in dataset:\", len(dataset.queries.items()))\n",
    "print(\"\\n----------- Examples of queries: -----------\\n\")\n",
    "example_queries = list(dataset.queries.values())[:3]\n",
    "for query in example_queries:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPaJz6i7r_8g",
    "outputId": "bf0f8323-8973-48c3-d409-cba9943cd24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in dataset: 11429\n",
      "Minimum Length (in words): 2\n",
      "Maximum Length (in words): 269\n",
      "Average Length (in words): 41.93\n",
      "\n",
      "----------- Examples of documents: -----------\n",
      "\n",
      "compact memories have flexible capacities  a digital data storage\n",
      "system with capacity up to bits and random and or sequential access\n",
      "is described\n",
      "\n",
      "an electronic analogue computer for solving systems of linear equations\n",
      "mathematical derivation of the operating principle and stability\n",
      "conditions for a computer consisting of amplifiers\n",
      "\n",
      "electronic coordinate transformer  circuit details are given for\n",
      "the construction of an electronic calculating unit which enables\n",
      "the polar coordinates of a vector modulus and cosine or sine of the\n",
      "argument to be derived from those of a rectangular system of axes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate document stats\n",
    "doc_lengths = [len(doc.split()) for doc in dataset.docs.values()]\n",
    "\n",
    "# Print examples of documents\n",
    "print(\"Total documents in dataset:\", len(dataset.docs.items()))\n",
    "print(f\"Minimum Length (in words): {min(doc_lengths)}\")\n",
    "print(f\"Maximum Length (in words): {max(doc_lengths)}\")\n",
    "print(f\"Average Length (in words): {(sum(doc_lengths) / len(doc_lengths) if doc_lengths else 0):.2f}\")\n",
    "print(\"\\n----------- Examples of documents: -----------\\n\")\n",
    "example_docs = list(dataset.docs.values())[:3]\n",
    "for doc in example_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mePEZwlr_8h",
    "outputId": "c86a1970-a6d3-4356-ae45-f6641ba06c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: MEASUREMENT OF DIELECTRIC CONSTANT OF LIQUIDS BY THE USE OF MICROWAVE TECHNIQUES\n",
      "\n",
      "-------------------------------\n",
      "Document:\n",
      "broadband millimetre wave paramagnetic resonance spectrometer  the\n",
      "specimen and waveguide which can be cooled by means of a cryostat\n",
      "are placed between close pole pieces giving high uniform magnetic\n",
      "fields  design details and some measurements on zero field splittings\n",
      "are given\n",
      "\n",
      "Document:\n",
      "microwave measurements of dielectric absorption in dilute solutions\n",
      "\n",
      "Document:\n",
      "dielectric properties of ice at very low frequencies and the influence\n",
      "of a polarizing field  measurements at frequencies down to are reported\n",
      "the loss factor passes through a low frequency maximum which is distinguishable\n",
      "from that associated with the dipole dispersion by its different\n",
      "temperature dependence  the effect of impurities is to shift the\n",
      "maximum towards higher frequencies  application of a unidirectional\n",
      "field does not affect the permittivity of the pure crystals but eliminates\n",
      "the low frequency dispersion when impurities are present  the\n",
      "observations are consistent with macdonalds theory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print example of one query and relevant documents\n",
    "query_rel_doc_ex_df = dataset.pairs.head(3)[[\"query_id\", \"doc_id\"]]\n",
    "query_id = query_rel_doc_ex_df[\"query_id\"].unique()[0]\n",
    "doc_ids = query_rel_doc_ex_df[\"doc_id\"]\n",
    "print(f\"Query: {dataset.queries[query_id]}\")\n",
    "print(\"-------------------------------\")\n",
    "for doc_id in doc_ids:\n",
    "    print(f\"Document:\\n{dataset.docs[doc_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvY9hwtRr_8h"
   },
   "source": [
    "### Paired Dataset Creation\n",
    "\n",
    "Activation patching relies on pairs of inputs, consisting of one *perturbed* input and one *baseline* input, where the *perturbed* input is constructed by applying some function to modify an *original* input (e.g., inserting a query term to the end of a document) and the *baseline* input is a padded variant of the *original* input to maintain token lengths between the pairs.\n",
    "\n",
    "In the main demo, we show one possible type of perturbation (appending a query term to the end of a document), but there are several other possible types of functions that could be use to generate the activation patching input pairs depending on what behavior you are trying to investigate. In this section, we discuss possible general perturbation methods, and describe some specific perturbations that we define aligning with IR axioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlzOPo-4r_8h"
   },
   "outputs": [],
   "source": [
    "# Helper function just to print\n",
    "def pretty_print_triplets(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    Pretty prints triplets of queries, documents, and their corresponding perturbed documents from a batch.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A dictionary containing 'queries', 'documents', and 'perturbed_documents' from a DataLoader.\n",
    "        tokenizer: The tokenizer used to decode the input IDs.\n",
    "    \"\"\"\n",
    "    # Get the queries, documents, and perturbed documents from the batch\n",
    "    queries = batch[\"queries\"]\n",
    "    documents = batch[\"documents\"]\n",
    "    perturbed_documents = batch[\"perturbed_documents\"]\n",
    "\n",
    "    # Loop through the batch size\n",
    "    for i in range(len(documents[\"input_ids\"])):\n",
    "        # Get the input IDs\n",
    "        query_ids = queries[\"input_ids\"][i]\n",
    "        original_ids = documents[\"input_ids\"][i]\n",
    "        perturbed_ids = perturbed_documents[\"input_ids\"][i]\n",
    "\n",
    "        # Decode the input IDs to text\n",
    "        query_decoded = tokenizer.decode(query_ids.tolist(), skip_special_tokens=False).replace(\"[PAD]\", \"\").strip()\n",
    "        original_doc_decoded = tokenizer.decode(original_ids.tolist(), skip_special_tokens=False).replace(\"[PAD]\", \"\").strip()\n",
    "        perturbed_doc_decoded = tokenizer.decode(perturbed_ids.tolist(), skip_special_tokens=False).replace(\"[PAD]\", \"\").strip()\n",
    "\n",
    "        # Pretty print\n",
    "        print(f\"Triplet {i + 1}:\")\n",
    "        print(\"Query:\", query_decoded)\n",
    "        print(\"Original Document:\", original_doc_decoded)\n",
    "        print(\"Perturbed Document:\", perturbed_doc_decoded)\n",
    "        print(\"=\" * 50)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h1rtEMatoXR"
   },
   "source": [
    "## Make your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Ht999zr_8h"
   },
   "outputs": [],
   "source": [
    "@perturbation\n",
    "def my_perturbation(text: str, query: str = None) -> str:\n",
    "    \"\"\"\n",
    "    A simple perturbation function that replaces the first word of the text with 'REPLACED'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be perturbed.\n",
    "\n",
    "    Returns:\n",
    "        str: The perturbed text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        words[0] = \"REPLACED\"\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KEsmNqcuKuk"
   },
   "source": [
    "# Using Perturbations\n",
    "\n",
    "Once you have a perturbation and a dataset, we provide collate functions which automatically apply your perturbation to your dataset and allow batching for more efficient experiments. A standard torch dataloader is all you need but remember that different architectures need different input formats so make sure to use the correct collate functon for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjASsJm5r_8i"
   },
   "outputs": [],
   "source": [
    "data_collator = DotDataCollator(model.tokenizer, my_perturbation)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1xQgcD4r_8i"
   },
   "outputs": [],
   "source": [
    "cat_data_collator = CatDataCollator(cat_model.tokenizer, my_perturbation)\n",
    "cat_dataloader = DataLoader(dataset, batch_size=1, collate_fn=cat_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbOf8RGBr_8i",
    "outputId": "6fc2f9ca-cd05-4a73-e9dd-4009b0b1e014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet 1:\n",
      "Query: [CLS] measurement of dielectric constant of liquids by the use of microwave techniques [SEP]\n",
      "Original Document: [CLS] broadband millimetre wave paramagnetic resonance spectrometer the specimen and waveguide which can be cooled by means of a cryostat are placed between close pole pieces giving high uniform magnetic fields design details and some measurements on zero field splittings are given X X X X X X X X X X X X X X [SEP]\n",
      "Perturbed Document: [CLS] measurement of dielectric constant of liquids by the use of microwave techniques broadband millimetre wave paramagnetic resonance spectrometer the specimen and waveguide which can be cooled by means of a cryostat are placed between close pole pieces giving high uniform magnetic fields design details and some measurements on zero field splittings are given [SEP]\n",
      "==================================================\n",
      "Triplet 1:\n",
      "Query: [CLS] measurement of dielectric constant of liquids by the use of microwave techniques [SEP]\n",
      "Original Document: [CLS] microwave measurements of dielectric absorption in dilute solutions X X X X X X X X X X X X X X [SEP]\n",
      "Perturbed Document: [CLS] measurement of dielectric constant of liquids by the use of microwave techniques microwave measurements of dielectric absorption in dilute solutions [SEP]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mechir/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2855: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(cat_data_collator):\n",
    "    pretty_print_triplets(batch, cat_model.tokenizer)\n",
    "\n",
    "    # stop after 2 batches\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuPZDLP4r_8i"
   },
   "source": [
    "### Verify Difference in Performance on Perturbed Pairs\n",
    "\n",
    "Before we can finalize our paired dataset and proceedto the patching experiments, there's a couple things to check:\n",
    "- Does the chosen perturbation even have an effect on model behavior?\n",
    "- If yes, what is that effect? (i.e., Do the *baseline* or *perturbed* inputs have a higher relevance score on average?)\n",
    "\n",
    "So first, let's calculate the performances of the three toy perturbations we defined earlier (prepend, append, replace) and plot their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgPaKaKmr_8i"
   },
   "outputs": [],
   "source": [
    "# Helper function for plotting difference in performance\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_score_dists(baseline_scores, perturbed_scores, type=\"hist\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    if type == \"hist\":\n",
    "        plt.hist(baseline_scores, label='Baseline', color='blue')\n",
    "        plt.hist(perturbed_scores, label='Perturbed', color='orange')\n",
    "        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.ylabel('Frequency')\n",
    "    elif type == \"box\":\n",
    "        plt.boxplot([baseline_scores, perturbed_scores], labels=['Baseline', 'Perturbed'])\n",
    "        plt.ylabel('Scores')\n",
    "\n",
    "    plt.xlabel('Scores')\n",
    "    plt.title('Distribution of Baseline vs Perturbed Scores')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYvnhDmqr_8j"
   },
   "outputs": [],
   "source": [
    "baseline_performance, perturbed_performance = [], []\n",
    "for i, batch in enumerate(cat_dataloader):\n",
    "    # Get the queries, documents, and perturbed documents from the batch\n",
    "    sequences = batch[\"sequences\"]\n",
    "    perturbed_sequences = batch[\"perturbed_sequences\"]\n",
    "\n",
    "    baseline_scores = model.forward(**sequences) # [batch_size x 1]\n",
    "    perturbed_scores = model.forward(**perturbed_sequences) # [batch_size x 1]\n",
    "\n",
    "    baseline_performance += baseline_scores.flatten().tolist()\n",
    "    perturbed_performance += perturbed_scores.flatten().tolist()\n",
    "\n",
    "    # stop after 2 batches\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE3pic6kr_8j"
   },
   "outputs": [],
   "source": [
    "plot_score_dists(baseline_performance, perturbed_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF7N9Kder_8j"
   },
   "outputs": [],
   "source": [
    "plot_score_dists(baseline_performance, perturbed_performance, type=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS=10\n",
    "patching_head_outputs = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # Get the queries, documents, and perturbed documents from the batch\n",
    "    sequences = batch[\"sequences\"]\n",
    "    perturbed_sequences = batch[\"perturbed_sequences\"]\n",
    "    \n",
    "    patch_head_out = cat_model.patch(sequences, perturbed_sequences, patch_type=\"head_all\")\n",
    "    patching_head_outputs.append(patch_head_out)\n",
    "    \n",
    "    if i == ITERS:\n",
    "        break\n",
    "mean_head_outputs = torch.mean(torch.stack(patching_head_outputs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(mean_head_outputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mechir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
